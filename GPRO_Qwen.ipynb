{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GycA6DWk8aQL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# First install unsloth and vllm\n",
        "!pip install unsloth vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "_7SFgtlQiQFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Remove all PIL-related modules from memory\n",
        "modules = list(sys.modules.keys())\n",
        "for module_name in modules:\n",
        "    if \"PIL\" in module_name or \"torch\" in module_name or \"unsloth\" in module_name:\n",
        "        sys.modules.pop(module_name)"
      ],
      "metadata": {
        "id": "JPX1JW5a8tOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 64 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "bLzQH-P_iNAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import Dataset\n",
        "\n",
        "# Your disaster_samples\n",
        "disaster_samples = [\n",
        "    {\n",
        "        \"request\": \"URGENT: Flood in Downtown Manila. 15 families trapped on rooftops. Need rescue boats. Contact: Maria 09171234567\",\n",
        "        \"reasoning\": \"This is a flood emergency in Manila with people trapped. The urgency is critical given people are on rooftops. They need immediate rescue with boats. Contact information is provided.\",\n",
        "        \"extraction\": {\n",
        "            \"location\": \"Downtown Manila\",\n",
        "            \"disaster_type\": \"flood\",\n",
        "            \"urgency_level\": \"critical\",\n",
        "            \"people_affected\": \"15 families\",\n",
        "            \"specific_needs\": [\"rescue boats\"],\n",
        "            \"contact_info\": \"Maria 09171234567\",\n",
        "            \"time_mentioned\": \"urgent\",\n",
        "            \"additional_details\": \"families trapped on rooftops\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"request\": \"House fire at 123 Oak Street. Two elderly residents need help evacuating. Call John 555-0123.\",\n",
        "        \"reasoning\": \"This is a house fire with specific address. Two elderly people need evacuation help. Contact provided. High urgency due to fire and vulnerable population.\",\n",
        "        \"extraction\": {\n",
        "            \"location\": \"123 Oak Street\",\n",
        "            \"disaster_type\": \"house fire\",\n",
        "            \"urgency_level\": \"high\",\n",
        "            \"people_affected\": \"2 elderly residents\",\n",
        "            \"specific_needs\": [\"evacuation assistance\"],\n",
        "            \"contact_info\": \"John 555-0123\",\n",
        "            \"time_mentioned\": \"not specified\",\n",
        "            \"additional_details\": \"elderly residents unable to evacuate alone\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"request\": \"Building collapsed in Kathmandu. 20 people trapped under debris. Can hear voices. Need heavy rescue equipment ASAP.\",\n",
        "        \"reasoning\": \"Building collapse in Kathmandu with people trapped. Critical situation as people are heard under debris. Need specialized rescue equipment immediately.\",\n",
        "        \"extraction\": {\n",
        "            \"location\": \"Kathmandu\",\n",
        "            \"disaster_type\": \"building collapse\",\n",
        "            \"urgency_level\": \"critical\",\n",
        "            \"people_affected\": \"20 people trapped\",\n",
        "            \"specific_needs\": [\"heavy rescue equipment\", \"trained rescue personnel\"],\n",
        "            \"contact_info\": \"not provided\",\n",
        "            \"time_mentioned\": \"ASAP\",\n",
        "            \"additional_details\": \"voices heard from under debris\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Prompt format\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Util functions\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "# Main dataset loader\n",
        "def get_disaster_dataset() -> Dataset:\n",
        "    entries = []\n",
        "    for sample in disaster_samples:\n",
        "        reasoning = sample[\"reasoning\"]\n",
        "        answer = sample[\"extraction\"]\n",
        "        answer_str = \"\\n\".join(f\"{k}: {v}\" for k, v in answer.items())\n",
        "        prompt = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": sample[\"request\"]}\n",
        "        ]\n",
        "        full_answer = XML_COT_FORMAT.format(reasoning=reasoning, answer=answer_str)\n",
        "        entries.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"answer\": full_answer\n",
        "        })\n",
        "    return Dataset.from_list(entries)\n",
        "\n",
        "# Now you can use this in your training loop\n",
        "dataset = get_disaster_dataset()\n"
      ],
      "metadata": {
        "id": "0QCmXodoiRGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n"
      ],
      "metadata": {
        "id": "olj4yfcniY8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from trl import GRPOConfig, GRPOTrainer\n",
        "# training_args = GRPOConfig(\n",
        "#     use_vllm = True, # use vLLM for fast inference!\n",
        "#     learning_rate = 5e-6,\n",
        "#     adam_beta1 = 0.9,\n",
        "#     adam_beta2 = 0.99,\n",
        "#     weight_decay = 0.1,\n",
        "#     warmup_ratio = 0.1,\n",
        "#     lr_scheduler_type = \"cosine\",\n",
        "#     optim = \"adamw_8bit\",\n",
        "#     logging_steps = 1,\n",
        "#     bf16 = is_bfloat16_supported(),\n",
        "#     fp16 = not is_bfloat16_supported(),\n",
        "#     per_device_train_batch_size = 1,\n",
        "#     gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "#     num_generations = 8, # Decrease if out of memory\n",
        "#     max_prompt_length = 256,\n",
        "#     max_completion_length = 200,\n",
        "#     # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "#     max_steps = 250,\n",
        "#     save_steps = 250,\n",
        "#     max_grad_norm = 0.1,\n",
        "#     report_to = \"none\", # Can use Weights & Biases\n",
        "#     output_dir = \"outputs\",\n",
        "# )\n",
        "\n",
        "from trl import GRPOConfig\n",
        "from torch.cuda import is_bf16_supported\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm=False,  # ✅ Disable vLLM for fast local test unless needed\n",
        "    learning_rate=1e-5,  # Slightly higher for quick convergence\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.01,  # Lower for small test\n",
        "    warmup_ratio=0.0,  # Disable warmup for quick tests\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    optim=\"adamw_torch\",  # Simplest optimizer for tests\n",
        "    logging_steps=1,\n",
        "    bf16=is_bf16_supported(),\n",
        "    fp16=not is_bf16_supported(),\n",
        "\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    num_generations=2,  # ✅ Reduce to save VRAM\n",
        "    max_prompt_length=128,  # Reduce for faster tokenization\n",
        "    max_completion_length=100,\n",
        "\n",
        "    max_steps=10,  # ✅ Only 10 steps for quick test\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,  # Don’t clutter output dir\n",
        "    max_grad_norm=0.1,\n",
        "\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"outputs/test\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "pSRNqSeWicxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "CnMd95hFifIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "SJIHhAmSijlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {
        "id": "yaxUYiiEikRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\n",
        "    \"NisalDeZoysa/disaster-response-grpo-qwen\",  # ✅ Correct format\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "tx_EHsa9im7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}